{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Text dosyaları data isimli klasör içerisinde olsun\n",
    "BASE_DIR = os.getcwd()\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data')"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Data klasörü içerisindeki .txt uzantılı dosyalar mevcut\n",
    "bu dosyaların içerisindeki her bir satırın eğitim verisinin bir elamanı olduğunu varsayıyoruz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "egitim_X = []\n",
    "#data klasöründeki tüm dosyaları teker teker oku\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    fname = os.path.join(TEXT_DATA_DIR, name)\n",
    "    #dosya uzantısı .txt ise \n",
    "    if fname.endswith(\".txt\"):\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                #satır başı ve sonu dışındaki olası boşlukları temizle\n",
    "                line = line.strip()\n",
    "                try:\n",
    "                  egitim_X.append(line)\n",
    "                except Exception as e:\n",
    "                  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korkma, sönmez bu şafaklarda yüzen al sancak;\n'Medeniyet!' dediğin tek dişi kalmış canavar?\n16\n"
     ]
    }
   ],
   "source": [
    "#ilk ve son satırları yazdır\n",
    "print egitim_X[0]\n",
    "print egitim_X[-1]\n",
    "\n",
    "#veri sayısını yazdır\n",
    "print len(egitim_X)"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "her satır bir giriş verisi olarak egitim_X listesine aktarıldı \n",
    "bunun yerine dosyalarda geçen bütün metinler birbirine eklenebilirdi \n",
    "Fakat bu şekilde hem metin ön işlemeyi hem de vektör dönüşümünü bir arada ilerletebiliriz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.preprocessing.text.Tokenizer(num_words=None, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ \n",
    "', lower=True, split=' ', char_level=False, oov_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokinizer nesnesini oluştur\n",
    "\n",
    "# parametres opsiyonları ve default değerleri\n",
    "# num_words=None, lower=True, split=' ', char_level=False, oov_token=None\n",
    "# filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ '\n",
    "\n",
    "# metinlerde en çok geçen 10 kelimenin işleme alınmasını sağlayalım\n",
    "MAX_NUM_WORDS = 10\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giriş verisine göre tokinizer sınıfının ayarlanması\n",
    "tokenizer.fit_on_texts(egitim_X)\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Şu anda giriş verimiz hala egitim_X 'de sakladığımız şekilde\n",
    "Fakat artık tokinizer giriş verisiyle ilgili bilmemiz gereken bir çok şeyi biliyor\n",
    "Bunlar:\n",
    "- kelime listesi\n",
    "- kelime frekansları\n",
    "- en fazla geçen 10 kelime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celal dağları Çatma aşarım canavar olayım zincir zırhlı kahraman üstünde sancak yüzen sönmez dediğin korkma dökülen bana serhaddim helal bir sana nasıl en imanı gibiyim yıldızıdır kükremiş ey sel afakını ' gül yurdumun çehreni bu hakkıdır olmaz sonra sığmam milletimin o benimdir dolu sönmeden tüten böyle son hür tek kanlarımız 'medeniyet ocak hangi duvar var yırtarım boğar iman bendimi ırkıma gibi ulusun hilal ben vuracakmış taşarım ancak dişi çiğner ezelden parlayacak al nazlı enginlere kalmış göğsüm kurban ne istiklal beridir hakk'a benim garbın sarmışsa yaşadım Şaşarım çılgın şafaklarda şiddet çelik yaşarım tapan milletimindir\n"
     ]
    }
   ],
   "source": [
    "#hangi kelimelerin geçtiğine bakalım\n",
    "for kelime in tokenizer.word_index:\n",
    "    print kelime,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam  93  adet farklı kelime bulunmaktadır\nToplam  10  adet  kelime işleme alınacaktır\n"
     ]
    }
   ],
   "source": [
    "print \"Toplam \", len(tokenizer.word_index), \" adet farklı kelime bulunmaktadır\"\n",
    "print \"Toplam \", tokenizer.num_words, \" adet  kelime işleme alınacaktır\""
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Görüldüğü üzere bütün kelimeler default olarak verilmiş split=' ' kuralına göre bulunmuş\n",
    "Kelimeler küçük harflerle yazılmış. İstisna olarak Türkçe karakterler büyük harf olarak kalmış.\n",
    "Bunun sebebi keras preprocessing kütüphanesinin karakter kodunu çözememesidir. \n",
    "\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Bu problem ilgili aşamada line = line.decode('utf-8') satırı eklenerek çözülebilir"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Tokenizer sınıfının en önemli nesneleri word_index ve word_counts'dır."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korkma kelimesi toplam  2  kez geçmiştir\nkorkma kelimesine verilen id =  4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"korkma kelimesi toplam \", tokenizer.word_counts['korkma'], \" kez geçmiştir\"\n",
    "print \"korkma kelimesine verilen id = \", tokenizer.word_index['korkma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celal = 1      dağları = 1      Çatma = 1      aşarım = 1      canavar = 1      olayım = 1      zincir = 1      zırhlı = 1      kahraman = 1      üstünde = 1      sancak = 1      yüzen = 1      sönmez = 1      dediğin = 1      korkma = 2      dökülen = 1      bana = 1      serhaddim = 1      helal = 1      bir = 2      sana = 1      nasıl = 1      en = 1      imanı = 1      gibiyim = 1      yıldızıdır = 1      kükremiş = 1      ey = 1      sel = 1      afakını = 1      ' = 1      gül = 1      yurdumun = 1      çehreni = 1      bu = 3      hakkıdır = 1      olmaz = 1      sonra = 1      sığmam = 1      milletimin = 2      o = 3      benimdir = 1      dolu = 1      sönmeden = 1      tüten = 1      böyle = 1      son = 1      hür = 2      tek = 1      kanlarımız = 1      'medeniyet = 1      ocak = 1      hangi = 1      duvar = 1      var = 1      yırtarım = 1      boğar = 1      iman = 1      bendimi = 1      ırkıma = 1      gibi = 1      ulusun = 1      hilal = 1      ben = 1      vuracakmış = 1      taşarım = 1      ancak = 1      dişi = 1      çiğner = 1      ezelden = 1      parlayacak = 1      al = 1      nazlı = 1      enginlere = 1      kalmış = 1      göğsüm = 1      kurban = 1      ne = 1      istiklal = 1      beridir = 1      hakk'a = 1      benim = 3      garbın = 1      sarmışsa = 1      yaşadım = 1      Şaşarım = 1      çılgın = 1      şafaklarda = 1      şiddet = 1      çelik = 1      yaşarım = 1      tapan = 1      milletimindir = 1     \n"
     ]
    }
   ],
   "source": [
    "#bütün kelimeleri ve frekanslarını yazdıralım\n",
    "for kelime in tokenizer.word_index:\n",
    "    print kelime, \"=\", tokenizer.word_counts[kelime], \"    \","
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Bu aşamaya kadar gerçekleştirilen bütün adımlar, giriş verisinin sayısal veriler olarak temsil edilmesi içindi. Şimdi bunu gerçekleştirelim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(egitim_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 1, 9]    []    [2, 3, 5]    [2, 2, 3]    []    [6, 1, 1]    []    [5]    [7, 7]    []    []    []    []    [3]    [4, 6]    []   \n"
     ]
    }
   ],
   "source": [
    "for satir in sequences:\n",
    "    print satir, \"  \","
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Veriler artık cümleler yerine sayısal değerler ifade edilmiş\n",
    "Fakat önemli eksikler var\n",
    "Bazı cümlelerin [] şeklinde boş dizilere dönüştüğünü görüyoruz\n",
    "Genellik tamamen boş diziyle karşılaşmak burdakindan daha nadir gerçekleşir. Çünki en azından en çok geçen 10.000 kelime'den birkaçı bir veride bulunması beklenir. Fakat bu örnekte, verileri manuel olarak takip edebilmemiz açısından az sayıda veri kullanıldı ve maksimum kelime sayısı 10 seçildi."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Ayrıca görüldüğü üzere her veri farklı uzunlukta. Çoğu algoritma giriş verilerinin sabit uzunlukta olmasını bekler. Bu durumlarda tokenizer'ın padding metodunu kullanırız."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# her bir giriş verisininin uzunluğu yalnızca 4 olsun\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "giris = pad_sequences(sequences, maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 8 1 9]    [0 0 0 0]    [0 2 3 5]    [0 2 2 3]    [0 0 0 0]    [0 6 1 1]    [0 0 0 0]    [0 0 0 5]    [0 0 7 7]    [0 0 0 0]    [0 0 0 0]    [0 0 0 0]    [0 0 0 0]    [0 0 0 3]    [0 0 4 6]    [0 0 0 0]   \n"
     ]
    }
   ],
   "source": [
    "for satir in giris:\n",
    "    print satir, \"  \","
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Artık giriş verisi belirlediğimiz metin ön işleme şartlarına göre hazır durumda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
