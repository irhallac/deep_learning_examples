{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "#Text dosyaları data isimli klasör içerisinde olsun\n",
    "BASE_DIR = os.getcwd()\n",
    "TEXT_DATA_DIR = os.path.join(BASE_DIR, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "egitim_X = []\n",
    "#data klasöründeki tüm dosyaları teker teker oku\n",
    "for name in sorted(os.listdir(TEXT_DATA_DIR)):\n",
    "    fname = os.path.join(TEXT_DATA_DIR, name)\n",
    "    #dosya uzantısı .txt ise \n",
    "    if fname.endswith(\".txt\"):\n",
    "        with open(fname) as infile:\n",
    "            for line in infile:\n",
    "                #satır başı ve sonu dışındaki olası boşlukları temizle\n",
    "                line = line.strip()\n",
    "                try:\n",
    "                  egitim_X.append(line)\n",
    "                except Exception as e:\n",
    "                  print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Korkma, sönmez bu şafaklarda yüzen al sancak;\n",
      "'Medeniyet!' dediğin tek dişi kalmış canavar?\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "#ilk ve son satırları yazdır\n",
    "print egitim_X[0]\n",
    "print egitim_X[-1]\n",
    "\n",
    "#veri sayısını yazdır\n",
    "print len(egitim_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokinizer nesnesini oluştur\n",
    "\n",
    "# parametres opsiyonları ve default değerleri\n",
    "# num_words=None, lower=True, split=' ', char_level=False, oov_token=None\n",
    "# filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~ '\n",
    "\n",
    "# metinlerde en çok geçen 10 kelimenin işleme alınmasını sağlayalım\n",
    "MAX_NUM_WORDS = 10\n",
    "tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# giriş verisine göre tokinizer sınıfının ayarlanması\n",
    "tokenizer.fit_on_texts(egitim_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celal dağları Çatma aşarım canavar olayım zincir zırhlı kahraman üstünde sancak yüzen sönmez dediğin korkma dökülen bana serhaddim helal bir sana nasıl en imanı gibiyim yıldızıdır kükremiş ey sel afakını ' gül yurdumun çehreni bu hakkıdır olmaz sonra sığmam milletimin o benimdir dolu sönmeden tüten böyle son hür tek kanlarımız 'medeniyet ocak hangi duvar var yırtarım boğar iman bendimi ırkıma gibi ulusun hilal ben vuracakmış taşarım ancak dişi çiğner ezelden parlayacak al nazlı enginlere kalmış göğsüm kurban ne istiklal beridir hakk'a benim garbın sarmışsa yaşadım Şaşarım çılgın şafaklarda şiddet çelik yaşarım tapan milletimindir\n"
     ]
    }
   ],
   "source": [
    "#hangi kelimelerin geçtiğine bakalım\n",
    "for kelime in tokenizer.word_index:\n",
    "    print kelime,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toplam  93  adet farklı kelime bulunmaktadır\n",
      "Toplam  10  adet  kelime işleme alınacaktır\n"
     ]
    }
   ],
   "source": [
    "print \"Toplam \", len(tokenizer.word_index), \" adet farklı kelime bulunmaktadır\"\n",
    "print \"Toplam \", tokenizer.num_words, \" adet  kelime işleme alınacaktır\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "korkma kelimesi toplam  2  kez geçmiştir\n",
      "korkma kelimesine verilen id =  4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print \"korkma kelimesi toplam \", tokenizer.word_counts['korkma'], \" kez geçmiştir\"\n",
    "print \"korkma kelimesine verilen id = \", tokenizer.word_index['korkma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "celal = 1      dağları = 1      Çatma = 1      aşarım = 1      canavar = 1      olayım = 1      zincir = 1      zırhlı = 1      kahraman = 1      üstünde = 1      sancak = 1      yüzen = 1      sönmez = 1      dediğin = 1      korkma = 2      dökülen = 1      bana = 1      serhaddim = 1      helal = 1      bir = 2      sana = 1      nasıl = 1      en = 1      imanı = 1      gibiyim = 1      yıldızıdır = 1      kükremiş = 1      ey = 1      sel = 1      afakını = 1      ' = 1      gül = 1      yurdumun = 1      çehreni = 1      bu = 3      hakkıdır = 1      olmaz = 1      sonra = 1      sığmam = 1      milletimin = 2      o = 3      benimdir = 1      dolu = 1      sönmeden = 1      tüten = 1      böyle = 1      son = 1      hür = 2      tek = 1      kanlarımız = 1      'medeniyet = 1      ocak = 1      hangi = 1      duvar = 1      var = 1      yırtarım = 1      boğar = 1      iman = 1      bendimi = 1      ırkıma = 1      gibi = 1      ulusun = 1      hilal = 1      ben = 1      vuracakmış = 1      taşarım = 1      ancak = 1      dişi = 1      çiğner = 1      ezelden = 1      parlayacak = 1      al = 1      nazlı = 1      enginlere = 1      kalmış = 1      göğsüm = 1      kurban = 1      ne = 1      istiklal = 1      beridir = 1      hakk'a = 1      benim = 3      garbın = 1      sarmışsa = 1      yaşadım = 1      Şaşarım = 1      çılgın = 1      şafaklarda = 1      şiddet = 1      çelik = 1      yaşarım = 1      tapan = 1      milletimindir = 1     \n"
     ]
    }
   ],
   "source": [
    "#bütün kelimeleri ve frekanslarını yazdıralım\n",
    "for kelime in tokenizer.word_index:\n",
    "    print kelime, \"=\", tokenizer.word_counts[kelime], \"    \","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(egitim_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 8, 1, 9]    []    [2, 3, 5]    [2, 2, 3]    []    [6, 1, 1]    []    [5]    [7, 7]    []    []    []    []    [3]    [4, 6]    []   \n"
     ]
    }
   ],
   "source": [
    "for satir in sequences:\n",
    "    print satir, \"  \","
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# her bir giriş verisininin uzunluğu yalnızca 4 olsun\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "giris = pad_sequences(sequences, maxlen=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4 8 1 9]    [0 0 0 0]    [0 2 3 5]    [0 2 2 3]    [0 0 0 0]    [0 6 1 1]    [0 0 0 0]    [0 0 0 5]    [0 0 7 7]    [0 0 0 0]    [0 0 0 0]    [0 0 0 0]    [0 0 0 0]    [0 0 0 3]    [0 0 4 6]    [0 0 0 0]   \n"
     ]
    }
   ],
   "source": [
    "for satir in giris:\n",
    "    print satir, \"  \","
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
